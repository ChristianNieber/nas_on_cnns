{
  //NAS algorithm parameters for multiple runs. Start main_20_runs.py to run.
"EVOLUTIONARY": {

    // Experiment name is used for folder name where all files are stored. This is overridden if main_20_runs.py is used
    "experiment_name" : "Stepper-Adaptive",

    // initial population, "lenet, "perceptron" or "random"
    "initial_individuals" : "random",

    // allow resume from saved state
    "resume": true,

    // can resume at a certain generation (50, 100, ...)
    "resume_generation": 0,

    // random seed for Random and numpy modules, -1 for none. This is incremented before each run if main_multiple_runs.py is used
    "random_seed" : 100,

    //Maximum number of generations
    "num_generations": 200,

    // my - number of parents
    "my" : 1,

    //lambda - number of offspring
    "lambda": 5,

    //comma strategy means no parents in selection
    "comma_strategy": false,

    // NAS strategy to use: "Random", "F-DENSER", "Stepper-Decay" or "Stepper-Adaptive"
    "nas_strategy": "Stepper-Adaptive",

    // 0, or maximum number of trainable parameters of a generated architecture. A larger architecture is considered invalid for NAS. 0 for no limit.
    "max_parameters": 0,

   // initial step width for Stepper-Decay and Stepper-Adaptive
   "stepper_initial_sigma" : 0.5
},

  //network specific parameters
  "NETWORK": {
    //structure of the layers groups of the network [[non-terminal, min, max], ...], i.e. we have 1 to 10 feature layers followed by 1 to 5 classification layers
    "network_structure": [["features", 1, 10], ["classification", 1, 5]],

    //output layer is added at the end
    "output": "output",

    //macro blocks: currently only learning
    "macro_structure": ["learning"],

    //number of layers for random initialisation
    "network_structure_init": {"features":[2,3,4], "classification":[1]}
  },

  //training parameters
  "TRAINING": {
    // use cache file for storing and retrieving evaluated network configurations
    // The cache file path is relative to the data file path. A global cache for all experiments with 10 epochs is used.
    "use_evaluation_cache": true,
    "evaluation_cache_file": "../cache_10.pkl",

    //maximum training time for each network architecture (in seconds)
    "max_training_time": 10,
    //maximum training epochs for each network architecture
    "max_training_epochs": 10,

    // use data augmentation (defined in data_augmentation.py)
    "use_data_augmentation": false
  }
}